{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167d8fe-9b34-4dfb-8ced-057a1ca53913",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7adf65-95b9-475a-b25f-c492a2a286d3",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing\n",
    "For simplicity, we'll use a very basic form of preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00007a-853e-46c6-9d5e-67a981243f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def data_preprocessor(source_sentences, target_sentences):\n",
    "    \n",
    "    source_tokenizer = Tokenizer()\n",
    "    source_tokenizer.fit_on_texts(source_sentences)\n",
    "    source_sequences = source_tokenizer.texts_to_sequences(source_sentences)\n",
    "    source_padded = pad_sequences(source_sequences, padding='post')\n",
    "      \n",
    "    target_tokenizer = Tokenizer()\n",
    "    target_tokenizer.fit_on_texts(target_sentences)\n",
    "    target_sequences = target_tokenizer.texts_to_sequences(target_sentences)\n",
    "    target_padded = pad_sequences(target_sequences, padding='post')\n",
    "    \n",
    "    return source_padded, target_padded, source_tokenizer, target_tokenizer\n",
    "\n",
    "english_sentences = ['hello', 'world', 'how are you', 'I am fine', 'have a good day']\n",
    "spanish_sentences = ['hola', 'mundo', 'cómo estás', 'estoy bien', 'ten un buen día']\n",
    "input_texts, target_texts, source_tokenizer, target_tokenizer = data_preprocessor(english_sentences, spanish_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61be34-12c9-4f7f-a26b-866c28b1f4fb",
   "metadata": {},
   "source": [
    "# Step 2: Building the Model\n",
    "Next, we construct the seq2seq model with an attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdaf87e-3d10-4d3d-96c2-f971d8ece277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate\n",
    "from tensorflow.keras.layers import AdditiveAttention as Attention\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 256\n",
    "latent_dim = 512\n",
    "num_encoder_tokens = len(source_tokenizer.word_index) + 1\n",
    "num_decoder_tokens = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(num_encoder_tokens, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(num_decoder_tokens, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "# Attention Layer\n",
    "attention = Attention()\n",
    "attention_output = attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Concatenating attention output and decoder LSTM output \n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_output])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8df58c-a91c-4ea7-8d0a-1d79caf07492",
   "metadata": {},
   "source": [
    "# Step 3: Training the Model\n",
    "We convert the target texts to categorical data for training. Note that in a real scenario, you should use more data and perform train-test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00abc042-fc51-45f4-90fe-391fe796c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "decoder_target_data = to_categorical(target_texts, num_decoder_tokens)\n",
    "model.fit([input_texts, target_texts], decoder_target_data, batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85b3a2-826d-45c2-a4f8-716bd013969a",
   "metadata": {},
   "source": [
    "# Step 4: Inference Model\n",
    "Set up the inference models for the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164efa6b-1273-41e7-bb9f-8720ff25c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Inference Model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder Inference Model\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81344b9-0b28-43f9-a892-e9a6f6030c43",
   "metadata": {},
   "source": [
    "# Step 5: Translation Function\n",
    "Finally, let's create a function for the translation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0dd95-98ec-4bc7-b0be-99b7f3337df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_text):\n",
    "    # Tokenize and pad the input sequence\n",
    "    input_seq = source_tokenizer.texts_to_sequences([input_text])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=input_texts.shape[1], padding='post')\n",
    "\n",
    "    # Get the encoder states\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate an empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first character of the target sequence with the start character\n",
    "    target_seq[0, 0] = target_tokenizer.word_index['start']  # Assuming 'start' is the start token\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = target_tokenizer.index_word[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length or find stop token.\n",
    "        if (sampled_char == 'end' or len(decoded_sentence) > 50):  # Assuming 'end' is the end token\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "# Example usage\n",
    "translated_sentence = translate(\"hello\")\n",
    "print(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad510935-6213-4885-aefc-32baa5c5b2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
